{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1c7b50",
   "metadata": {},
   "source": [
    "# Convolutional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43461127",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d796aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368442dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line)\n",
    "    # X_pad = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=(0, 0))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a82f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 9, 9, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 9, 9, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 3)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1, 1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, :, :, 0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, :, :, 0])\n",
    "zero_pad_test(zero_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bea86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    s = np.multiply(a_slice_prev, W)     # a_slice_prev and W HAVE THE SAME SIZE, no shifting needed\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z + float(b)\n",
    "\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17c1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n",
      "\u001b[92mAll tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40969/2124633778.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z = Z + float(b)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)\n",
    "conv_single_step_test(conv_single_step)\n",
    "\n",
    "assert (type(Z) == np.float64), \"You must cast the output to numpy float 64\"\n",
    "assert np.isclose(Z, -6.999089450680221), \"Wrong value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01509003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "  \"\"\"\n",
    "  Implements the forward propagation for a convolution function\n",
    "  \n",
    "  Arguments:\n",
    "  A_prev -- output activations of the previous layer, \n",
    "      numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "  W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "  b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "  hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "      \n",
    "  Returns:\n",
    "  Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "  cache -- cache of values needed for the conv_backward() function\n",
    "  \"\"\"\n",
    "  \n",
    "  # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "  # (m, n_H_prev, n_W_prev, n_C_prev) = None\n",
    "\n",
    "  # Retrieve dimensions from W's shape (≈1 line)\n",
    "  # (f, f, n_C_prev, n_C) = None\n",
    "  # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "  # stride = None\n",
    "  # pad = None\n",
    "  \n",
    "  # Compute the dimensions of the CONV output volume using the formula given above. \n",
    "  # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n",
    "  # n_H = None\n",
    "  # n_W = None\n",
    "  \n",
    "  # Initialize the output volume Z with zeros. (≈1 line)\n",
    "  # Z = None\n",
    "  \n",
    "  # Create A_prev_pad by padding A_prev\n",
    "  # A_prev_pad = None\n",
    "  \n",
    "  # for i in range(None):               # loop over the batch of training examples\n",
    "      # a_prev_pad = None               # Select ith training example's padded activation\n",
    "      # for h in range(None):           # loop over vertical axis of the output volume\n",
    "          # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "          # vert_start = None\n",
    "          # vert_end = None\n",
    "          \n",
    "          # for w in range(None):       # loop over horizontal axis of the output volume\n",
    "              # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
    "              # horiz_start = None\n",
    "              # horiz_end = None\n",
    "              \n",
    "              # for c in range(None):   # loop over channels (= #filters) of the output volume\n",
    "                                      \n",
    "                  # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                  # a_slice_prev = None\n",
    "                  \n",
    "                  # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
    "                  # weights = None\n",
    "                  # biases = None\n",
    "                  # Z[i, h, w, c] = None\n",
    "\n",
    "  (m, n_H_prev, n_W_prev, _n_C_prev) = (A_prev.shape[0], A_prev.shape[1], A_prev.shape[2], A_prev.shape[3])\n",
    "  \n",
    "  (f, f, _n_C_prev, n_C) = (W.shape[0], W.shape[1], W.shape[2], W.shape[3])\n",
    "  \n",
    "  stride = hparameters['stride']\n",
    "  \n",
    "  pad = hparameters['pad']\n",
    "  \n",
    "  n_H = int((n_H_prev + 2 * pad - f) / stride + 1)\n",
    "  \n",
    "  n_W = int((n_W_prev + 2 * pad - f) / stride + 1)\n",
    "  \n",
    "  Z = np.zeros(shape=(m, n_H, n_W, n_C))\n",
    "  \n",
    "  A_prev_pad = zero_pad(A_prev, pad)\n",
    "  \n",
    "\n",
    "  for i in range(m):\n",
    "    a_prev_pad = A_prev_pad[i]          # Select ith training example's padded activation\n",
    "\n",
    "    for h in range(n_H):                    # loop over vertical axis of the output volume\n",
    "      vert_start = h * stride\n",
    "      vert_end = vert_start + f\n",
    "\n",
    "      for w in range(n_W):              # loop over horizontal axis of the output volume\n",
    "        horiz_start = w * stride        # Find the horizontal start and end of the current \"slice\"\n",
    "        horiz_end = horiz_start + f\n",
    "      \n",
    "        for c in range(n_C):            # loop over channels (= #filters) of the output volume\n",
    "          # Use the corners to define the (3D) slice of a_prev_pad for example i\n",
    "          # A_prev_pad is 4D because it contains 3D matrices for m examples\n",
    "          a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]     # use the padded version so we don't loose information!\n",
    "\n",
    "          weights = W[:, :, :, c]\n",
    "          biases = b[:, :, :, c]\n",
    "          Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
    "          \n",
    "            \n",
    "  assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "  \n",
    "  # Save information in \"cache\" for the backprop\n",
    "  cache = (A_prev, W, b, hparameters)\n",
    "  \n",
    "  return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74dc710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.5511276474566768\n",
      "Z[0,2,1] =\n",
      " [-2.17796037  8.07171329 -0.5772704   3.36286738  4.48113645 -2.89198428\n",
      " 10.99288867  3.03171932]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n",
      "\u001b[92mFirst Test: All tests passed!\n",
      "\u001b[92mSecond Test: All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40969/2124633778.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z = Z + float(b)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 7, 4)\n",
    "W = np.random.randn(3, 3, 4, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "z_mean = np.mean(Z)\n",
    "z_0_2_1 = Z[0, 2, 1]\n",
    "cache_0_1_2_3 = cache_conv[0][1][2][3]\n",
    "print(\"Z's mean =\\n\", z_mean)\n",
    "print(\"Z[0,2,1] =\\n\", z_0_2_1)\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_0_1_2_3)\n",
    "\n",
    "conv_forward_test_1(z_mean, z_0_2_1, cache_0_1_2_3)\n",
    "conv_forward_test_2(conv_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41bb814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev                          # Pooling doesn't change the output!\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. \n",
    "                    # Use an if statement to differentiate the modes. \n",
    "                    # Use np.max and np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    #assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7166df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "\n",
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[1.96710175 0.84616065 1.27375593]\n",
      " [1.96710175 0.84616065 1.23616403]\n",
      " [1.62765075 1.12141771 1.2245077 ]]\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[ 0.44497696 -0.00261695 -0.31040307]\n",
      " [ 0.50811474 -0.23493734 -0.23961183]\n",
      " [ 0.11872677  0.17255229 -0.22112197]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pool_forward_test_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mA.shape = \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(A.shape))\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mA[1, 1] =\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, A[\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mpool_forward_test_1\u001b[49m(pool_forward)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Case 2: stride of 2\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[0mCASE 2:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pool_forward_test_1' is not defined"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "print(\"CASE 1:\\n\")\n",
    "np.random.seed(1)\n",
    "A_prev_case_1 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_1 = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "\n",
    "pool_forward_test_1(pool_forward)\n",
    "\n",
    "# Case 2: stride of 2\n",
    "print(\"\\n\\033[0mCASE 2:\\n\")\n",
    "np.random.seed(1)\n",
    "A_prev_case_2 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_2 = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[0] =\\n\", A[0])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1] =\\n\", A[1])\n",
    "\n",
    "pool_forward_test_2(pool_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65ff6e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[1.96710175 0.84616065 1.27375593]\n",
      " [1.96710175 0.84616065 1.23616403]\n",
      " [1.62765075 1.12141771 1.2245077 ]]\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[ 0.44497696 -0.00261695 -0.31040307]\n",
      " [ 0.50811474 -0.23493734 -0.23961183]\n",
      " [ 0.11872677  0.17255229 -0.22112197]]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "\n",
    "pool_forward_test(pool_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bfa11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[0] =\n",
      " [[[1.74481176 0.90159072 1.65980218]\n",
      "  [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      " [[1.74481176 0.90159072 1.65980218]\n",
      "  [1.74481176 1.6924546  1.65980218]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[1] =\n",
      " [[[ 0.42944926  0.08446996 -0.27290905]\n",
      "  [ 0.15077452  0.28911175  0.00123239]]\n",
      "\n",
      " [[ 0.42944926  0.08446996 -0.27290905]\n",
      "  [ 0.15077452  0.28911175  0.00123239]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[0] =\\n\", A[0])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1] =\\n\", A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed52bd",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8016effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"    \n",
    "    \n",
    "        \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros_like(A_prev)                          \n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros_like(b)\n",
    "    \n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "           for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "               for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05eeaca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n",
      "\u001b[92m All tests passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40969/2124633778.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z = Z + float(b)\n"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "\n",
    "assert type(dA) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert type(dW) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert type(db) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert dA.shape == (10, 4, 4, 3), f\"Wrong shape for dA  {dA.shape} != (10, 4, 4, 3)\"\n",
    "assert dW.shape == (2, 2, 3, 8), f\"Wrong shape for dW {dW.shape} != (2, 2, 3, 8)\"\n",
    "assert db.shape == (1, 1, 1, 8), f\"Wrong shape for db {db.shape} != (1, 1, 1, 8)\"\n",
    "assert np.isclose(np.mean(dA), 1.4524377), \"Wrong values for dA\"\n",
    "assert np.isclose(np.mean(dW), 1.7269914), \"Wrong values for dW\"\n",
    "assert np.isclose(np.mean(db), 7.8392325), \"Wrong values for db\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3b18a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"    \n",
    "    # (≈1 line)\n",
    "    # mask = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    mask = x == np.max(x)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67e51828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2, 3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)\n",
    "\n",
    "x = np.array([[-1, 2, 3],\n",
    "              [2, -3, 2],\n",
    "              [1, 5, -2]])\n",
    "\n",
    "y = np.array([[False, False, False],\n",
    "     [False, False, False],\n",
    "     [False, True, False]])\n",
    "mask = create_mask_from_window(x)\n",
    "\n",
    "assert type(mask) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert mask.shape == x.shape, \"Input and output shapes must match\"\n",
    "assert np.allclose(mask, y), \"Wrong output. The True value must be at position (2, 1)\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bc84ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"    \n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz/(n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6840dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2, 2))\n",
    "print('distributed value =', a)\n",
    "\n",
    "\n",
    "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert a.shape == (2, 2), f\"Wrong shape {a.shape} != (2, 2)\"\n",
    "assert np.sum(a) == 2, \"Values must sum to 2\"\n",
    "\n",
    "a = distribute_value(100, (10, 10))\n",
    "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert a.shape == (10, 10), f\"Wrong shape {a.shape} != (10, 10)\"\n",
    "assert np.sum(a) == 100, \"Values must sum to 100\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "105c6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m): # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "        \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        \n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value da from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        \n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "914f236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 2, 2)\n",
      "(5, 5, 3, 2)\n",
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev1[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev2[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(A.shape)\n",
    "print(cache[0].shape)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev1 = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev1[1,1] = ', dA_prev1[1, 1])  \n",
    "print()\n",
    "dA_prev2 = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev2[1,1] = ', dA_prev2[1, 1]) \n",
    "\n",
    "assert type(dA_prev1) == np.ndarray, \"Wrong type\"\n",
    "assert dA_prev1.shape == (5, 5, 3, 2), f\"Wrong shape {dA_prev1.shape} != (5, 5, 3, 2)\"\n",
    "assert np.allclose(dA_prev1[1, 1], [[0, 0], \n",
    "                                    [ 5.05844394, -1.68282702],\n",
    "                                    [ 0, 0]]), \"Wrong values for mode max\"\n",
    "assert np.allclose(dA_prev2[1, 1], [[0.08485462,  0.2787552], \n",
    "                                    [1.26461098, -0.25749373], \n",
    "                                    [1.17975636, -0.53624893]]), \"Wrong values for mode average\"\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7bcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

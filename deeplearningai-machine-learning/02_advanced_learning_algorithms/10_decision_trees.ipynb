{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361544e7",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f597ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import decision_trees_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73edf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(p: float):\n",
    "  \"\"\"\n",
    "    Computes the Entropy H for a given value of p (probability; the proportion of positive examples in the SPLIT)\n",
    "  \"\"\"\n",
    "  if p == 0 or p == 1:\n",
    "    return 0\n",
    "  else:\n",
    "    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7244487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_right_left(X, feature_index):\n",
    "  \"\"\"\n",
    "  Given a feature at index `feature_index` in a matrix X of shape (n_examples, n_features), split_right_left creates 2 groups:\n",
    "    - one with the indices of the dataset (rows) where the feature is present\n",
    "    - one with the indices of the dataset where the feature is absent\n",
    "  \"\"\"\n",
    "  yes_indices = []\n",
    "  no_indices = []\n",
    "\n",
    "  for i, x in enumerate(X):\n",
    "    if x[feature_index] == 1:\n",
    "      yes_indices.append(i)\n",
    "    else:\n",
    "      no_indices.append(i)\n",
    "  \n",
    "  return yes_indices, no_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c81256fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_entropy_for_node(X, y, yes_indices, no_indices):\n",
    "  w_left = len(yes_indices) / len(X)\n",
    "  w_right = len(no_indices) / len(X)\n",
    "\n",
    "  # actual target values of all the left values\n",
    "  p_left = sum(y[yes_indices]) / len(yes_indices)\n",
    "  p_right = sum(y[no_indices]) / len(no_indices)\n",
    "\n",
    "  w_entropy = w_left * compute_entropy(p_left) + w_right * compute_entropy(p_right)\n",
    "  return w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12cf035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, yes_indices, no_indices):\n",
    "  p_node = sum(y) / len(y)\n",
    "  h_node = compute_entropy(p_node)\n",
    "  weighted_entropy = compute_weighted_entropy_for_node(X, y, yes_indices, no_indices)\n",
    "  i_gain = h_node - weighted_entropy\n",
    "\n",
    "  return i_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e74c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 1, 1],\n",
    "[0, 0, 1],\n",
    " [0, 1, 0],\n",
    " [1, 0, 1],\n",
    " [1, 1, 1],\n",
    " [1, 1, 0],\n",
    " [0, 0, 0],\n",
    " [1, 1, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0]])\n",
    "\n",
    "# cat - not cat\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])\n",
    "\n",
    "feature_names = ['Ear Shape', 'Face Shape', 'Whiskers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab8d8b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting by Ear Shape would result in an information gain of 0.2780719051126377\n",
      "Splitting by Face Shape would result in an information gain of 0.034851554559677034\n",
      "Splitting by Whiskers would result in an information gain of 0.12451124978365313\n",
      "Best feature to split by next: Ear Shape\n"
     ]
    }
   ],
   "source": [
    "i_gains = []\n",
    "\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "  yes_indices, no_indices = split_right_left(X_train, i)  # For each feature splits the dataset in \"have this feature\" and \"don't have this feature\"\n",
    "  information_gain = compute_information_gain(X_train, y_train, yes_indices, no_indices)\n",
    "  print(f'Splitting by {feature_name} would result in an information gain of {information_gain}')\n",
    "  i_gains.append(information_gain)\n",
    "\n",
    "i_best = np.argmax(i_gains)\n",
    "print(f'Best feature to split by next: {feature_names[i_best]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb57130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef48f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
